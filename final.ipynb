{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import PunktSentenceTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pyap\n",
    "import time\n",
    "import textstat\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence_tokenizer(text):\n",
    "    tokenizer = PunktSentenceTokenizer()\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def word_tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def find_emoticons(text):\n",
    "    return re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "\n",
    "def find_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if word in stop_words]\n",
    "\n",
    "def find_address(text):\n",
    "    address = pyap.parse(text, country='US')\n",
    "    return [str(addr) for addr in address]\n",
    "\n",
    "def find_phone_numbers(text):\n",
    "    return re.findall(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)\n",
    "\n",
    "def find_account_numbers(text):\n",
    "    return re.findall(r'\\b\\d{9}\\b', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])\n",
    "    return text\n",
    "\n",
    "def stem_lem(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [stemmer.stem(lemmatizer.lemmatize(word)) for word in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df.columns = ['0', 'Id', 'Timestamp', 'Query', 'User', 'Text']\n",
    "\n",
    "train_df_cleaned = train_df.copy()\n",
    "\n",
    "## Mark time\n",
    "normal_start = time.time()\n",
    "\n",
    "train_df['Sentences'] = train_df['Text'].apply(sentence_tokenizer)\n",
    "train_df['Words'] = train_df['Text'].apply(word_tokenizer)\n",
    "train_df['Emoticons'] = train_df['Text'].apply(find_emoticons)\n",
    "train_df['StopWords'] = train_df['Words'].apply(find_stop_words)\n",
    "train_df['Addresses'] = train_df['Text'].apply(find_address)\n",
    "train_df['PhoneNumbers'] = train_df['Text'].apply(find_phone_numbers)\n",
    "train_df['AccountNumbers'] = train_df['Text'].apply(find_account_numbers)\n",
    "\n",
    "normal_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_start = time.time()\n",
    "\n",
    "train_df_cleaned['Text'] = train_df_cleaned['Text'].apply(preprocess_text)\n",
    "\n",
    "train_df_cleaned['Sentences'] = train_df_cleaned['Text'].apply(sentence_tokenizer)\n",
    "train_df_cleaned['Words'] = train_df_cleaned['Text'].apply(word_tokenizer)\n",
    "train_df_cleaned['Emoticons'] = train_df_cleaned['Text'].apply(find_emoticons)\n",
    "train_df_cleaned['StopWords'] = train_df_cleaned['Words'].apply(find_stop_words)\n",
    "train_df_cleaned['Addresses'] = train_df_cleaned['Text'].apply(find_address)\n",
    "train_df_cleaned['PhoneNumbers'] = train_df_cleaned['Text'].apply(find_phone_numbers)\n",
    "train_df_cleaned['AccountNumbers'] = train_df_cleaned['Text'].apply(find_account_numbers)\n",
    "\n",
    "train_df_cleaned['Words'] = train_df_cleaned['Words'].apply(stem_lem)\n",
    "cleaned_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_sent_length': 1.7032660645412903, 'max_sent_length': 64, 'min_sent_length': 1, 'sent_count': 2725224, 'word_count': 26247410, 'vocab_size': 874199, 'max_word_length': 136, 'num_emoticons': 14450, 'num_stop_words': 7556112, 'num_lowercase': 18166684, 'num_special_chars': 27811922, 'num_addresses': 731, 'num_phone_numbers': 946, 'num_account_numbers': 114, 'processing_time': 315.4849579334259}\n"
     ]
    }
   ],
   "source": [
    "stats_before = {\n",
    "    'avg_sent_length': train_df['Sentences'].apply(len).mean(),\n",
    "    'max_sent_length': train_df['Sentences'].apply(len).max(),\n",
    "    'min_sent_length': train_df['Sentences'].apply(len).min(),\n",
    "    'sent_count': train_df['Sentences'].apply(len).sum(),\n",
    "    'word_count': train_df['Words'].apply(len).sum(),\n",
    "    'vocab_size': len(set([word for words in train_df['Words'] for word in words])),\n",
    "    'max_word_length': max([len(word) for words in train_df['Words'] for word in words]),\n",
    "    'num_emoticons': train_df['Emoticons'].apply(len).sum(),\n",
    "    'num_stop_words': train_df['StopWords'].apply(len).sum(),\n",
    "    'num_lowercase': train_df['Words'].apply(lambda x: len([word for word in x if word.islower()])).sum(),\n",
    "    'num_special_chars': train_df['Text'].apply(lambda x: len([char for char in x if not char.isalnum()])).sum(),\n",
    "    'num_addresses': train_df['Addresses'].apply(len).sum(),\n",
    "    'num_phone_numbers': train_df['PhoneNumbers'].apply(len).sum(),\n",
    "    'num_account_numbers': train_df['AccountNumbers'].apply(len).sum(),\n",
    "    'processing_time': normal_end - normal_start\n",
    "    \n",
    "}\n",
    "\n",
    "print(stats_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg_sent_length': 0.9997556248472655, 'max_sent_length': 1, 'min_sent_length': 0, 'sent_count': 1599608, 'word_count': 12350113, 'vocab_size': 726849, 'max_word_length': 123, 'num_emoticons': 0, 'num_stop_words': 6053, 'num_lowercase': 12350113, 'num_special_chars': 10698244, 'num_addresses': 15, 'num_phone_numbers': 0, 'num_account_numbers': 0, 'processing_time': 1176.6899199485779}\n"
     ]
    }
   ],
   "source": [
    "stats_after = {\n",
    "    'avg_sent_length': train_df_cleaned['Sentences'].apply(len).mean(),\n",
    "    'max_sent_length': train_df_cleaned['Sentences'].apply(len).max(),\n",
    "    'min_sent_length': train_df_cleaned['Sentences'].apply(len).min(),\n",
    "    'sent_count': train_df_cleaned['Sentences'].apply(len).sum(),\n",
    "    'word_count': train_df_cleaned['Words'].apply(len).sum(),\n",
    "    'vocab_size': len(set([word for words in train_df_cleaned['Words'] for word in words])),\n",
    "    'max_word_length': max([len(word) for words in train_df_cleaned['Words'] for word in words]),\n",
    "    'num_emoticons': train_df_cleaned['Emoticons'].apply(len).sum(),\n",
    "    'num_stop_words': train_df_cleaned['StopWords'].apply(len).sum(),\n",
    "    'num_lowercase': train_df_cleaned['Words'].apply(lambda x: len([word for word in x if word.islower()])).sum(),\n",
    "    'num_special_chars': train_df_cleaned['Text'].apply(lambda x: len([char for char in x if not char.isalnum()])).sum(),\n",
    "    'num_addresses': train_df_cleaned['Addresses'].apply(len).sum(),\n",
    "    'num_phone_numbers': train_df_cleaned['PhoneNumbers'].apply(len).sum(),\n",
    "    'num_account_numbers': train_df_cleaned['AccountNumbers'].apply(len).sum(),\n",
    "    'processing_time': cleaned_end - cleaned_start\n",
    "}\n",
    "\n",
    "print(stats_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------------------+---------------------+\n",
      "|        Stats        |       Before       |       After        |         Diff        |\n",
      "+---------------------+--------------------+--------------------+---------------------+\n",
      "| Avg Sentence Length | 1.7032660645412903 | 0.9997556248472655 | -0.7035104396940247 |\n",
      "| Max Sentence Length |         64         |         1          |         -63         |\n",
      "| Min Sentence Length |         1          |         0          |          -1         |\n",
      "|    Sentence Count   |      2725224       |      1599608       |       -1125616      |\n",
      "|      Word Count     |      26247410      |      12350113      |      -13897297      |\n",
      "|      Vocab Size     |       874199       |       726849       |       -147350       |\n",
      "|   Max Word Length   |        136         |        123         |         -13         |\n",
      "|    Num Emoticons    |       14450        |         0          |        -14450       |\n",
      "|    Num Stop Words   |      7556112       |        6053        |       -7550059      |\n",
      "| Num Lowercase Words |      18166684      |      12350113      |       -5816571      |\n",
      "|  Num Special Chars  |      27811922      |      10698244      |      -17113678      |\n",
      "|    Num Addresses    |        731         |         15         |         -716        |\n",
      "|  Num Phone Numbers  |        946         |         0          |         -946        |\n",
      "| Num Account Numbers |        114         |         0          |         -114        |\n",
      "|   Processing Time   | 315.4849579334259  | 1176.6899199485779 |   861.204962015152  |\n",
      "+---------------------+--------------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "## Output Stats\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Stats', 'Before', 'After', 'Diff']\n",
    "table.add_row(['Avg Sentence Length', stats_before['avg_sent_length'], stats_after['avg_sent_length'], stats_after['avg_sent_length'] - stats_before['avg_sent_length']])\n",
    "table.add_row(['Max Sentence Length', stats_before['max_sent_length'], stats_after['max_sent_length'], stats_after['max_sent_length'] - stats_before['max_sent_length']])\n",
    "table.add_row(['Min Sentence Length', stats_before['min_sent_length'], stats_after['min_sent_length'], stats_after['min_sent_length'] - stats_before['min_sent_length']])\n",
    "table.add_row(['Sentence Count', stats_before['sent_count'], stats_after['sent_count'], stats_after['sent_count'] - stats_before['sent_count']])\n",
    "table.add_row(['Word Count', stats_before['word_count'], stats_after['word_count'], stats_after['word_count'] - stats_before['word_count']])\n",
    "table.add_row(['Vocab Size', stats_before['vocab_size'], stats_after['vocab_size'], stats_after['vocab_size'] - stats_before['vocab_size']])\n",
    "table.add_row(['Max Word Length', stats_before['max_word_length'], stats_after['max_word_length'], stats_after['max_word_length'] - stats_before['max_word_length']])\n",
    "table.add_row(['Num Emoticons', stats_before['num_emoticons'], stats_after['num_emoticons'], stats_after['num_emoticons'] - stats_before['num_emoticons']])\n",
    "table.add_row(['Num Stop Words', stats_before['num_stop_words'], stats_after['num_stop_words'], stats_after['num_stop_words'] - stats_before['num_stop_words']])\n",
    "table.add_row(['Num Lowercase Words', stats_before['num_lowercase'], stats_after['num_lowercase'], stats_after['num_lowercase'] - stats_before['num_lowercase']])\n",
    "table.add_row(['Num Special Chars', stats_before['num_special_chars'], stats_after['num_special_chars'], stats_after['num_special_chars'] - stats_before['num_special_chars']])\n",
    "table.add_row(['Num Addresses', stats_before['num_addresses'], stats_after['num_addresses'], stats_after['num_addresses'] - stats_before['num_addresses']])\n",
    "table.add_row(['Num Phone Numbers', stats_before['num_phone_numbers'], stats_after['num_phone_numbers'], stats_after['num_phone_numbers'] - stats_before['num_phone_numbers']])\n",
    "table.add_row(['Num Account Numbers', stats_before['num_account_numbers'], stats_after['num_account_numbers'], stats_after['num_account_numbers'] - stats_before['num_account_numbers']])\n",
    "table.add_row(['Processing Time', stats_before['processing_time'], stats_after['processing_time'], stats_after['processing_time'] - stats_before['processing_time']])\n",
    "print(\"Table 1: Stats Before and After Preprocessing: - indicates decreased value\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reading Ease: 66.28469337793335\n",
      "Average Grade Level: 5.922668264167667\n",
      "Lexical Diversity: 0.05885363154167091\n"
     ]
    }
   ],
   "source": [
    "avg_reading_ease = train_df_cleaned['Text'].apply(textstat.flesch_reading_ease).mean()\n",
    "avg_grade_level = train_df_cleaned['Text'].apply(textstat.flesch_kincaid_grade).mean()\n",
    "lexical_diversity = stats_after['vocab_size'] / stats_after['word_count']\n",
    "\n",
    "print(f'Average Reading Ease: {avg_reading_ease}')\n",
    "print(f'Average Grade Level: {avg_grade_level}')\n",
    "print(f'Lexical Diversity: {lexical_diversity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Run Time: 5.26 minutes\n",
      "Cleaned Run Time: 19.61 minutes\n",
      "Total Run Time: 24.87 minutes\n"
     ]
    }
   ],
   "source": [
    "## Run Time\n",
    "\n",
    "print(f'Normal Run Time: {round((normal_end - normal_start) / 60, 2)} minutes')\n",
    "print(f'Cleaned Run Time: {round((cleaned_end - cleaned_start) / 60, 2)} minutes')\n",
    "print(f'Total Run Time: {round((cleaned_end - normal_start) / 60, 2)} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
